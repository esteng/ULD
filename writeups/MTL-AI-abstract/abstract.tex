\documentclass[12pt,letterpaper]{article}
\usepackage{natbib}
\usepackage{xcolor}

\newif\ifcomments
\commentstrue

\newcounter{note}
\stepcounter{note}
\newcommand{\n}[1]{\textcolor{red}{$^{\mathrm{n}}$\footnotemark[\arabic{note}]}\footnotetext[\arabic{note}]{\textcolor{red}{NOTE:
      #1}}\stepcounter{note}}
\newcommand{\cm}[1]{\textcolor{purple}{\ifcomments[COMMENT: #1]\else\fi}}
\newcommand{\q}[1]{\textcolor{green}{\ifcomments[QUESTION: #1]\else\fi}}
\newcommand{\td}[1]{\textcolor{gray}{\ifcomments[TODO: #1]\else\fi}}
\renewcommand{\d}[1]{\textcolor{lime}{\ifcomments[DELETED: #1]\else\fi}}
\newcommand{\e}[2]{\textcolor{lime}{\ifcomments[DELETED: #1]}\textcolor{red}{[REPLACED: #2]\else\fi}}
\renewcommand{\i}[1]{\textcolor{red}{\ifcomments[INSERT:]\else\fi}}

\title{\vspace{-1.5cm} Abstract}          
\author{Elias Stengel-Eskin, Emily Kellison-Linn, Timothy J. O'Donnell}

\begin{document}
\maketitle
%motivation
Language use is a defining characteristic of humans, and forms the foundation of both our interpersonal and human-computer interactions.  Understanding computational processes behind natural language learning and use would afford us a better understanding of the human cognition and help in engineering better human-computer interfaces. While many cutting-edge technologies exist for computational natural language processing, they tend to rely on large quantities training data, which often needs to be labelled\textemdash a costly and time-consuming process.
% \cm{This is not strictly true. It is definitely true for most of the deep-learning and NN-based approaches, and some of the statistical ones. I would relativize this comment a bit. } 
In many cases, adequate data of either sort (labelled or unlabelled) for supervised learning does not exist, leading to a paucity of natural language research on entire language families and regions.
% \cm{Again, this is a fair point but impresice. Much of the NN based data is not that costly to produce since it is just text or recorded speech, the labeled stuff is costly of course. But there are two issues: labeled stuff is costly, but even unlabeled stuff is rare for certain languages and tasks.}
In addition, human language learning is unsupervised in the machine-learning sense\textemdash while acquiring language, humans infer linguistic structure and rules implicitly.
% \cm{I think it is fair to say that human learning is unsupervised in the ML sense.}, 
% \cm{I believe that this statement is true, but it is dangerous ground---its the kind of thing that more empiricist types take issue at and better to avoid staking claims like this. Everyone can agree that unsupervised learning is good, whether or not the amount of training data or reinforcement that kids receive is sufficient according to whatever criterior to learn whatever phenomena. }. 
Taken together, these observations motivate an unsupervised approach to language learning systems which relies on theoretical insights from linguistics and cognitive science. We implement a model for unsupervised learning of a complete hierarchy of linguistic units directly from acoustic data, building upwards from phonemes to morphemes, and then to words and full sentence structure. \\

% \cm{In an abstract, you can be more terse than the above. I would cut this down by about half. Just assert the problems with supervised approaches and the advantages of unsupervised learning. You also need to get to the actual thing we are doing by the end of the first paragraph---i.e., unsupervised learning of linguistic structure (phonemes, morphemes, words, sentence structures) directly from acoustic data} \\
% problem
A state-of-the-art model of this learning task is the unsupervised lexicon discovery model (ULD) by Lee, O'Donnell, and Glass (2015), which was the first to jointly model all three levels of the discovery process. In partlicular, ULD introduced a \textit{joint learning} framework which extended earlier work in phoneme clustering  (Lee and Glass, 2013), word and sentence segmentation (Goldwater et al., 2007), and morpheme discovery (O'Donnell, 2015). 
% \cm{this is a bit convoluted ``we attempt to model the problem..'' LOG is a model, not a problem. The problem is learning units at multiple grain sizes (phones, morphemes, etc.) directly from the acoustic input. You should get to that at the end of the first paragraph and then in the beginning here basically say ``a state of the art model of this is LOG''...}
 % \cm{I wouldn't introduce this in a ``while'' clause---it sounds apologetic and deflationary. Just say we were the first to jointly model these earlier tasks. }
\textit{Joint learning} can lead to synergistic interactions between the constituent parts and can improve performance (Johnson, 2008). However, training ULD requires solving a computationally intractable inference problem.
% \cm{relative preceding statement to \emph{this} model---not all bayesian models are intractable (though almost all are)---e.g., a simple gaussian with conjugate priors.} 
In the original implementation of ULD, the the inference was implemented using sampling-based approaches.
% \cm{slightly incorrect jargon---say that inference was implemented using sampling-based approaches}
However, sampling is slow and difficult to parallelize, making it impractical to train a large model such as ULD on the vast amout of spoken language data available. \\

% approach
We present a version of ULD which uses variational Bayesian inference in lieu of sampling. The variational method re-casts the inference challenge as an optimization problem, computing an objective function and iteratively maximizing it with respect to the data. This algorithm lends itself well to parallelization and converges faster than sampling, with minimal impact on model performance (Zhai 2012, 2014). \\

% \cm{This last aside is a bit too vague---what do you mean that they can outperform, what cost was implied in the beginning.}\\
Our model buils on two earlier variational frameworks
% \cm{say ``builds on'' rather than ``combines'' and ``earlier'' rather than ``existing''}
: the Dirichlet Process Hidden Markov Model (DPHMM) implemented by Ondel et al. (2016) based on the sampling version of the same model by Lee and Glass (2013), and the AdaptorGrammar model (AG), first introduced in Johnson et al. (2007), and implemented with variational methods by Zhai et al. (2014). The DPHMM segments a continuous stream of audio input, hypothesizing a sequence of maximally repeatable phone-like units.
% \cm{This is not exactly an accurate description of the DPHMM model, I would describe it at a higher level, like say what it does rather than how.} 
The AG model learns to store frequently reused units given an underlying grammatical structure. We use it to parse the phone-like units produced by the DPHMM into composite linguistic objects such as words and sentences. 
% \cm{not exactly. again, avoid CFG and the term memoize, and describe what it does---learning to store frequently reused units.} 
% \cm{don't mix the description of the old models and the new work like this, say them separately.}
The two models interact through a \textit{noisy channel}
 % \cm{if this is a first mention, use italics,  avoid quotes (especially scare quotes, they are annoying to many readers).} 
 model of edit operations on the sequence of phone-like units, which approximates a phonological system. Both the DPHMM and AG systems make use of Bayesian nonparametric processes which allow for variable numbers of phone, morpheme, and word classes while penalizing overly complex models.
 % \cm{well, the PYP and DP are really basically the same thing. Again, describe on a higher level ``bayesian nonparametrics''} 
The model is able to acquire the full set of items in the linguistic hierarchy under completely unsupervised conditions. Crucially, because of the nonparametric nature of the model, this acquisition is language-independent. Our variational framework allows the model to be distributed across multiple processors, run faster, and process larger datasets. This will result in the application of the model to more languages in a shorter timeframe, which is crucial not only to research but also to engineering applications.  \\
 % \cm{You can end this more strongly by saying instead that the model is able to acquire the full stack of linguistic units in a fully unsupervised, language-independent way. You also need to say significantly more about the novel contribution---building on this earlier work we develop a VB inference algorithm, which has thesesuch and such advantages.} \\
We will present some preliminary pilot results from running this model, as well as an overview of the model structure and function. We will also discuss some further extensions of the model.\cm{I need to think a little about what to say now about results....}

\end{document}

