\documentclass[12pt,letterpaper]{article}
\usepackage{natbib}
\usepackage{xcolor}

\newif\ifcomments
\commentstrue

\newcounter{note}
\stepcounter{note}
\newcommand{\n}[1]{\textcolor{red}{$^{\mathrm{n}}$\footnotemark[\arabic{note}]}\footnotetext[\arabic{note}]{\textcolor{red}{NOTE:
      #1}}\stepcounter{note}}
\newcommand{\cm}[1]{\textcolor{purple}{\ifcomments[COMMENT: #1]\else\fi}}
\newcommand{\q}[1]{\textcolor{green}{\ifcomments[QUESTION: #1]\else\fi}}
\newcommand{\td}[1]{\textcolor{gray}{\ifcomments[TODO: #1]\else\fi}}
\renewcommand{\d}[1]{\textcolor{lime}{\ifcomments[DELETED: #1]\else\fi}}
\newcommand{\e}[2]{\textcolor{lime}{\ifcomments[DELETED: #1]}\textcolor{red}{[REPLACED: #2]\else\fi}}
\renewcommand{\i}[1]{\textcolor{red}{\ifcomments[INSERT:]\else\fi}}

\title{\vspace{-1.5cm} Abstract}          
\author{Elias Stengel-Eskin, Emily Kellison-Linn, Timothy J. O'Donnell}

\begin{document}
\maketitle
%motivation
Language use is a defining characteristic of humans, and forms the foundation of both our interpersonal and human-computer interactions.  Understanding the computational processes behind natural language learning and use would afford us a better understanding of the human cognition and help in engineering better human-computer interfaces. While many cutting-edge technologies exist for computational natural language processing, they tend to rely on large quantities training data, which often needs to be labelled\textemdash a costly and time-consuming process.
% \cm{This is not strictly true. It is definitely true for most of the deep-learning and NN-based approaches, and some of the statistical ones. I would relativize this comment a bit. } 
In many cases, adequate data of either sort (labelled or unlabelled) for supervised learning does not exist, leading to a paucity of natural language processing research on entire language families and regions.
% \cm{Again, this is a fair point but impresice. Much of the NN based data is not that costly to produce since it is just text or recorded speech, the labeled stuff is costly of course. But there are two issues: labeled stuff is costly, but even unlabeled stuff is rare for certain languages and tasks.}
In addition, human language learning is unsupervised in the machine-learning sense\textemdash while acquiring language, humans infer linguistic structure and rules implicitly.
% \cm{I think it is fair to say that human learning is unsupervised in the ML sense.}, 
% \cm{I believe that this statement is true, but it is dangerous ground---its the kind of thing that more empiricist types take issue at and better to avoid staking claims like this. Everyone can agree that unsupervised learning is good, whether or not the amount of training data or reinforcement that kids receive is sufficient according to whatever criterior to learn whatever phenomena. }. 
Taken together, these observations motivate an unsupervised approach to language learning systems which builds on theoretical insights from linguistics and cognitive science. We implement a model for unsupervised learning of a complete hierarchy of linguistic units directly from acoustic data, building upwards from phonemes to morphemes, and then to words and full sentence structure. \\

% \cm{In an abstract, you can be more terse than the above. I would cut this down by about half. Just assert the problems with supervised approaches and the advantages of unsupervised learning. You also need to get to the actual thing we are doing by the end of the first paragraph---i.e., unsupervised learning of linguistic structure (phonemes, morphemes, words, sentence structures) directly from acoustic data} \\
% problem
A state-of-the-art model of this learning task is the unsupervised lexicon discovery model (ULD) by Lee, O'Donnell, and Glass (2015), which was the first system to jointly model phoneme, word, and sentence structure induction.\cm{It was not obvious that there are three levels. so I fixed.} In partlicular, ULD introduced a framework which extended earlier work by jointly learning acoustic representations of phonemes (Lee and Glass, 2013), word and morpheme inventories and boundaries in sentences (Goldwater et al., 2007), and a novel noisy-channel model representing symbolic variation in the relization of phonemes.\cm{actually, we don't use fragment grammars---that's on the list of todos for us. Also, the way this was worded made it sound like we introduced joint learning, but that is an old idea.}
% \cm{this is a bit convoluted ``we attempt to model the problem..'' LOG is a model, not a problem. The problem is learning units at multiple grain sizes (phones, morphemes, etc.) directly from the acoustic input. You should get to that at the end of the first paragraph and then in the beginning here basically say ``a state of the art model of this is LOG''...}
 % \cm{I wouldn't introduce this in a ``while'' clause---it sounds apologetic and deflationary. Just say we were the first to jointly model these earlier tasks. }
\textit{Joint learning} can lead to synergistic interactions between the constituent parts and can improve performance (Johnson, 2008).\cm{This preceding sentence seems out of place here since we don't say anything more interesting about synergies afterward. I would just delete.} However, training ULD requires solving a computationally intractable inference problem.\cm{This is also a somewhat weird thing to say, the problem is intractable, sampling is one solution to intractable problems, it is is just too slow in this case. Skipe right to saying that the original sampling implementation was too slow to run the simulations we wanted to. Maybe there is where you guys wanted to mention joint learning---with fasterinference we could explore more hypotheses there?}
% \cm{relative preceding statement to \emph{this} model---not all bayesian models are intractable (though almost all are)---e.g., a simple gaussian with conjugate priors.} 
In the original implementation of ULD, the the inference was implemented using sampling-based approaches.
% \cm{slightly incorrect jargon---say that inference was implemented using sampling-based approaches}
However, sampling is slow and difficult to parallelize, making it impractical to train a large model such as ULD on the vast amout of spoken language data available.
\cm{Also, we can't run all of the interesting linguistics experiments that we want to.} \\

% approach
We present a version of ULD which uses variational Bayesian inference in lieu of sampling. The variational method re-casts the inference challenge as an optimization problem, computing an objective function and iteratively maximizing it with respect to the data. This algorithm lends itself well to parallelization and converges faster than sampling, with minimal impact on model performance (Zhai 2012, 2014). \\

% \cm{This last aside is a bit too vague---what do you mean that they can outperform, what cost was implied in the beginning.}\\
Our model buils on two earlier variational frameworks
% \cm{say ``builds on'' rather than ``combines'' and ``earlier'' rather than ``existing''}
: the Dirichlet Process Hidden Markov Model (DPHMM)\cm{I know lots of people do this---but don't introduce an abbreviation that you aren't going to use, there is no need for it. Also, in general, avoid abbreviations as much as possible. This is one of those things that people pick up as a feature of academic writing, but as Steve Pinker beat into me, it just makes it hard on the reader who has to remember what all of the abbreviations mean. Only use them when you are going to use them many many times and it is easy to remember.} implemented by Ondel et al. (2016) based on the sampling version of the same model by Lee and Glass (2013), and the AdaptorGrammar model (AG), first introduced in Johnson et al. (2007), and implemented with variational methods by Zhai et al. (2014).\cm{If you are going to introduce the names of the models that ULD builds on, they should be introduced when ULD is described. Here we are already talking about inference and we shouldn't be introducing new models and THEN new inference algorithms for those models. Here we should be saying that Ondel/Zhai introduce new inference algorithms for the model components introduced before. Also, probably best to cite the cohen paper as well as zhai, because that is the original locus classicus for VB AG.}  The DPHMM segments a continuous stream of audio input, hypothesizing a sequence of maximally repeatable phone-like units.
% \cm{This is not exactly an accurate description of the DPHMM model, I would describe it at a higher level, like say what it does rather than how.} 
The AG model learns to store frequently reused units given an underlying grammatical structure. We use it to parse the phone-like units produced by the DPHMM into composite linguistic objects such as words and sentences. 
% \cm{not exactly. again, avoid CFG and the term memoize, and describe what it does---learning to store frequently reused units.} 
% \cm{don't mix the description of the old models and the new work like this, say them separately.}
The two models interact through a \textit{noisy channel}
 % \cm{if this is a first mention, use italics,  avoid quotes (especially scare quotes, they are annoying to many readers).} 
 model of edit operations on the sequence of phone-like units, which approximates a phonological system. Both the DPHMM and AG systems make use of Bayesian nonparametric processes which allow for variable numbers of phone, morpheme, and word classes while penalizing overly complex models.
 % \cm{well, the PYP and DP are really basically the same thing. Again, describe on a higher level ``bayesian nonparametrics''} 
The model is able to acquire the full set of items in the linguistic hierarchy under completely unsupervised conditions. Crucially, because of the nonparametric nature of the model, this acquisition is language-independent.\cm{Ok, sorry that I didn't catch this before, but these preceding sentences are all description of the model and belongs above, with that, before we start explaining the new inference. It is important to maintain roughly a single ``point'' per paragraph.} Our variational framework allows the model to be distributed across multiple processors, run faster, and process larger datasets.\cm{It occurs to me that the novel part of the ULD model was the noisy channel and the novel part of this work is our soon-to-be-developed VB noisy-channel inference. We can say that ``we develop efficient variational inference by building on the VB blah blah described for AGs in blah and for DPHMMS in bleen extending ths with a novel approach to variational inference for the noisy-blerg, blah''}
 This will result in the application of the model to more languages in a shorter timeframe, which is crucial not only to research but also to engineering applications.\cm{No need for this last sentence, it is sort of obvious.}  \\
 % \cm{You can end this more strongly by saying instead that the model is able to acquire the full stack of linguistic units in a fully unsupervised, language-independent way. You also need to say significantly more about the novel contribution---building on this earlier work we develop a VB inference algorithm, which has thesesuch and such advantages.} \\
We will present some preliminary pilot results from running this model, as well as an overview of the model structure and function. We will also discuss some further extensions of the model.\cm{Don't promise any results, just say we will present the algorithm for now.}

\end{document}

