\documentclass[12pt,letterpaper]{article}
\usepackage{natbib}

\newcommand\tab[1][1cm]{\hspace*{#1}}

\title{\vspace{-1.5cm} Abstract}          
\author{Elias Stengel-Eskin, Emily Kellison-Linn, Timothy J. O'Donnell}

\begin{document}
\maketitle
%motivation
\tab Language use is a defining characteristic of humans, and forms the foundation of both our  interpersonal and human-computer interactions. Researching natural language is a touchstone issue for scientific understanding and for engineering applications. In particular, understanding  computational processes behind natural language usage would afford us a better understanding of the human cognition and help us engineer better human-computer interfaces. While many cutting-edge techniques exist for computational natural language processing, most rely on large quantities of labelled training data. Such data is costly and time-consuming to produce, and therefore does not exist for most of the world's languages, leading to a paucity of natural language research on entire language families and regions. In addition, human language learning more closely resembles an unsupervised learning problem, since human children do not have access to a sufficient amount of reinforcement to explain their deep language understanding. Taken together, these observations motivate an unsupervised approach to language learning systems which relies on theoretical insights from linguistics and cognitive science. \\ 
% problem
\tab We attempt to model the problem of unsupervised lexicon discovery (ULD), originally presented in Lee, O'Donnell, and Glass (2015), which is framed as follows: given a continuous acoustic signal, how do language learners segment the signal into phonemes, morphemes, and finally, words. While separate unsupervised models have been developed for the three grouping tasks (acoustic signal to phonemes in Lee and Glass (2013), word segmentation in Goldwater et al. (2007), and morpheme discovery in O'Donnell (2015)), the Lee et al. (2015) model was the first to incorporate all three tasks into one system. Such a ``joint learning'' model can lead to synergistic interactions between the constituent parts and can improve performance, as in Johnson (2008). The ULD system is a Bayesian model, which allows  dynamic updating in light of evidence while concomitantly incorporating prior theoretical intuitions about a phenomenon. However, training a Bayesian model requires solving a computationally intractable inference problem. In the past, including in the original implementation of the ULD model, the solution is approximated via sampling. However, sampling is slow and difficult to parallelize, making it impractical to train the ULD model on the vast amout of spoken language data available. \\
% approach
\tab We present a version of the ULD model which uses variational Bayesian inference in lieu of sampling. The variational method re-casts the inference challenge as an optimization problem, computing an objective function and iteratively maximizing it with respect to the data. This algorithm lends itself well to parallelization and converges faster than sampling. In general, this increase does not come at a high cost\textemdash in fact, variational models can outperform sampling-based models (Zhai 2012, 2014).\\
\tab Our model combines two existing variational frameworks: the Dirichlet Process Hidden Markov Model (DPHMM) implemented by Ondel et al. (2016) based on the sampling version of the same model by Lee and Glass (2013), and the AdaptorGrammar model (AG), first introduced in Johnson et al. (2007), and implemented with variational methods by Zhai et al. (2014). The DPHMM uses Hidden Markov models to identify clusters of similar acoustic segments given a stream of audio input, and hypothesizes a sequence of maximally repeatable phone-like units. The AG model dynamically adjusts the probabilities of an underlying Probabilistic Context Free Grammar to memoize frequently reused linguistic objects\textemdash in this case, it parses frequently occurring phone sequences identified by the DPHMM into morphemes and words. The two models interact through a ``noisy channel'' model of edit operations on the phone sequence, which approximates a phonological system. Both the DPHMM and AG systems make use of stochastic processes (the Dirichlet process and the Pitman-Yor process, respectively) which allow for variable numbers of phone, morpheme, and word classes while penalizing overly complex models. Having flexibility in the number of discoverable units on the phonemic and morphemic level helps the model be language-independent, and is particularly crucial on the word level, since the stacking of suffixes and prefixes means there could be an unbounded number of words in a vocabulary. \\
\tab We will present some preliminary pilot results from running this model, as well as an overview of the model structure and function. We will also discuss some further extensions of the model. 

\end{document}

