\documentclass[12pt,letterpaper]{article}
\usepackage{natbib}
\usepackage{xcolor}

\newif\ifcomments
\commentstrue

\newcounter{note}
\stepcounter{note}
\newcommand{\n}[1]{\textcolor{red}{$^{\mathrm{n}}$\footnotemark[\arabic{note}]}\footnotetext[\arabic{note}]{\textcolor{red}{NOTE:
      #1}}\stepcounter{note}}
\newcommand{\cm}[1]{\textcolor{purple}{\ifcomments[COMMENT: #1]\else\fi}}
\newcommand{\q}[1]{\textcolor{green}{\ifcomments[QUESTION: #1]\else\fi}}
\newcommand{\td}[1]{\textcolor{gray}{\ifcomments[TODO: #1]\else\fi}}
\renewcommand{\d}[1]{\textcolor{lime}{\ifcomments[DELETED: #1]\else\fi}}
\newcommand{\e}[2]{\textcolor{lime}{\ifcomments[DELETED: #1]}\textcolor{red}{[REPLACED: #2]\else\fi}}
\renewcommand{\i}[1]{\textcolor{red}{\ifcomments[INSERT:]\else\fi}}

\title{\vspace{-1.5cm} Abstract}          
\author{Elias Stengel-Eskin, Emily Kellison-Linn, Timothy J. O'Donnell}

\begin{document}
\maketitle
%motivation
Language use is a defining characteristic of humans, and forms the foundation of both our interpersonal and human-computer interactions.  Understanding the computational processes behind natural language learning would afford us a better understanding of the human cognition and help in engineering better human-computer interfaces. While many cutting-edge technologies exist for computational natural language processing, they tend to rely on large quantities of training data, which often needs to be labelled\textemdash a costly and time-consuming process. In many cases, adequate data of either sort (labelled or unlabelled) for supervised learning does not exist, leading to a paucity of natural language processing research on entire language families and regions. In addition, human language learning is unsupervised in the machine-learning sense\textemdash while acquiring language, humans infer linguistic structure and rules implicitly. Taken together, these observations motivate an unsupervised approach to  language learning systems which builds on theoretical insights from linguistics and cognitive science. We implement a model for unsupervised learning of a complete hierarchy of linguistic units directly from acoustic data, building upwards from phonemes to morphemes, and then to words and full sentence structure. \\

% problem
A state-of-the-art model of this learning task is the unsupervised lexicon discovery model (ULD) by Lee, O'Donnell, and Glass (2015), which was the first system to jointly model phoneme, word, and sentence structure induction. In particular, ULD extended earlier work by introducing a complete learning framework integrating three main components. The Dirichlet Process Hidden Markov Model (DPHMM) (Lee and Glass, 2013) segments a continuous stream of audio input, hypothesizing a sequence of maximally repeatable phone-like units. The Adaptor Grammar model (Johnson et al., 2007) learns to store frequently reused units given an underlying grammatical structure and is used to parse the phone-like units produced by the DPHMM into composite linguistic objects such as words and sentences Finally, Lee et al. introduced a novel \textit{noisy channel} model of edit operations on the sequence of phone-like units, which approximates a phonological system. Both the DPHMM and Adaptor Grammar systems make use of Bayesian nonparametric processes which allow for variable numbers of phone, morpheme, and word classes while penalizing overly complex models. The model is able to acquire the full set of items in the linguistic hierarchy under completely unsupervised conditions. Crucially, because of the nonparametric nature of the model, this acquisition is language-independent. \\

In the original implementation of ULD, the inference required to train the model was implemented using sampling-based approaches. However, sampling is slow and difficult to parallelize, making it unworkable to train a large model such as ULD on the vast amount of spoken language data available. Additionally, the speed limitations of sampling make it impractical to run comprehensive experiments on different components of the model to investigate the linguistic insights being learned. \\

% approach
We present a version of ULD which uses variational Bayesian inference in lieu of sampling. The variational method re-casts the inference challenge as an optimization problem, computing an objective function and iteratively maximizing it with respect to the data. This algorithm lends itself well to parallelization and converges faster than sampling, with minimal impact on model performance (Zhai 2012, 2014). Our model builds on two earlier variational frameworks: The Ondel et al. (2016) variational implementation of Lee and Glass's DPHMM model, and the Adaptor Grammar model implemented with variational methods by Zhai et al. (2014) based on Cohen (2010). We develop an efficient variational system by building on the two models described above and adding a novel variational implementation of the noisy channel to mediate between the two components. Our variational framework allows the model to be distributed across multiple processors, run faster, and process larger datasets. \\

We will present an overview of the model structure and function, and will also discuss some further extensions of the model.

\end{document}

