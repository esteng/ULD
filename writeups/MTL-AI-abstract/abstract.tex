\documentclass[12pt,letterpaper]{article}
\usepackage{natbib}
\usepackage{xcolor}
\newcommand\tab[1][1cm]{\hspace*{#1}}
\newif\ifcomments
\commentstrue

\newcounter{note}
\stepcounter{note}
\newcommand{\n}[1]{\textcolor{red}{$^{\mathrm{n}}$\footnotemark[\arabic{note}]}\footnotetext[\arabic{note}]{\textcolor{red}{NOTE:
      #1}}\stepcounter{note}}
\newcommand{\cm}[1]{\textcolor{purple}{\ifcomments[COMMENT: #1]\else\fi}}
\newcommand{\q}[1]{\textcolor{green}{\ifcomments[QUESTION: #1]\else\fi}}
\newcommand{\td}[1]{\textcolor{gray}{\ifcomments[TODO: #1]\else\fi}}
\renewcommand{\d}[1]{\textcolor{lime}{\ifcomments[DELETED: #1]\else\fi}}
\newcommand{\e}[2]{\textcolor{lime}{\ifcomments[DELETED: #1]}\textcolor{red}{[REPLACED: #2]\else\fi}}
\renewcommand{\i}[1]{\textcolor{red}{\ifcomments[INSERT:]\else\fi}}

\title{\vspace{-1.5cm} Abstract}          
\author{Elias Stengel-Eskin, Emily Kellison-Linn, Timothy J. O'Donnell}

\begin{document}
\maketitle
%motivation
Language use is a defining characteristic of humans, and forms the foundation of both our interpersonal and human-computer interactions.  Understanding computational processes behind natural language learning and use would afford us a better understanding of the human cognition and help us engineer better human-computer interfaces. While many cutting-edge techniques exist for computational natural language processing, most rely on large quantities of labelled training data.\cm{This is not strictly true. It is definitely true for most of the deep-learning and NN-based approaches, and some of the statistical ones. I would relativize this comment a bit. } Such data is costly and time-consuming to produce, and therefore does not exist for most of the world's languages, leading to a paucity of natural language research on entire language families and regions.\cm{Again, this is a fair point but impresice. Much of the NN based data is not that costly to produce since it is just text or recorded speech, the labeled stuff is costly of course. But there are two issues: labeled stuff is costly, but even unlabeled stuff is rare for certain languages and tasks.}
In addition, human language learning more closely resembles an unsupervised learning problem\cm{I think it is fair to say that human learning is unsupervised in the ML sense.}, since human children do not have access to a sufficient amount of reinforcement to explain their deep language understanding\cm{I believe that this statement is true, but it is dangerous ground---its the kind of thing that more empiricist types take issue at and better to avoid staking claims like this. Everyone can agree that unsupervised learning is good, whether or not the amount of training data or reinforcement that kids receive is sufficient according to whatever criterior to learn whatever phenomena. }. Taken together, these observations motivate an unsupervised approach to language learning systems which relies on theoretical insights from linguistics and cognitive science.\cm{In an abstract, you can be more terse than the above. I would cut this down by about half. Just assert the problems with supervised approaches and the advantages of unsupervised learning. You also need to get to the actual thing we are doing by the end of the first paragraph---i.e., unsupervised learning of linguistic structure (phonemes, morphemes, words, sentence structures) directly from acoustic data} \\
% problem
\tab \cm{tabs unnecessary in abstract} We \d{attempt} to model the problem of unsupervised lexicon discovery (ULD), originally presented in Lee, O'Donnell, and Glass (2015), which is framed as follows: given a continuous acoustic signal, how do language learners segment the signal into phonemes, morphemes, and finally, words.\cm{this is a bit convoluted ``we attempt to model the problem..'' LOG is a model, not a problem. The problem is learning units at multiple grain sizes (phones, morphemes, etc.) directly from the acoustic input. You should get to that at the end of the first paragraph and then in the beginning here basically say ``a state of the art model of this is LOG''...}
 While separate unsupervised models have been developed for the three grouping tasks (acoustic signal to phonemes in Lee and Glass (2013), word segmentation in Goldwater et al. (2007)\cm{I wouldn't introduce this in a ``while'' clause---it sounds apologetic and deflationary. Just say we were the first to jointly model these earlier tasks. }, and morpheme discovery in O'Donnell (2015)), the Lee et al. (2015) model was the first to incorporate all three tasks into one system. Such a ``joint learning'' model can lead to synergistic interactions between the constituent parts and can improve performance, as in Johnson (2008).\cm{no ``as in'' just the citation.} The ULD system is a Bayesian model, which allows  dynamic updating in light of evidence while concomitantly incorporating prior theoretical intuitions about a phenomenon.\cm{preceding sentence is redundant. Just say that LOG introduce a bayesian joint model of these various earlier efforts.} However, training a Bayesian model requires solving a computationally intractable inference problem.\cm{relative preceding statement to \emph{this} model---not all bayesian models are intractable (though almost all are)---e.g., a simple gaussian with conjugate priors.} In the past, including in the original implementation of the ULD model, the solution is approximated via sampling.\cm{slightly incorrect jargon---say that inference was implemented using sampling-based approaches} However, sampling is slow and difficult to parallelize, making it impractical to train the ULD model on the vast amout of spoken language data available. \\
% approach
\tab We present a version of the ULD model which uses variational Bayesian inference in lieu of sampling. The variational method re-casts the inference challenge as an optimization problem, computing an objective function and iteratively maximizing it with respect to the data. This algorithm lends itself well to parallelization and converges faster than sampling. In general, this increase does not come at a high cost\textemdash in fact, variational models can outperform sampling-based models (Zhai 2012, 2014).\cm{This last aside is a bit too vague---what do you mean that they can outperform, what cost was implied in the beginning.}\\
\tab Our model combines two existing variational frameworks\cm{say ``builds on'' rather than ``combines'' and ``earlier'' rather than ``existing''}: the Dirichlet Process Hidden Markov Model (DPHMM) implemented by Ondel et al. (2016) based on the sampling version of the same model by Lee and Glass (2013), and the AdaptorGrammar model (AG), first introduced in Johnson et al. (2007), and implemented with variational methods by Zhai et al. (2014). The DPHMM uses Hidden Markov models to identify clusters of similar acoustic segments given a stream of audio input, and hypothesizes a sequence of maximally repeatable phone-like units.\cm{This is not exactly an accurate description of the DPHMM model, I would describe it at a higher level, like say what it does rather than how.} The AG model dynamically adjusts the probabilities of an underlying Probabilistic Context Free Grammar\cm{not exactly. again, avoid CFG and the term memoize, and describe what it does---learning to store frequently reused units.} to memoize frequently reused linguistic objects\textemdash in this case,\cm{don't mix the description of the old models and the new work like this, say them separately.} it parses frequently occurring phone sequences identified by the DPHMM into morphemes and words. The two models interact through a ``noisy channel''\cm{if this is a first mention, use italics,  avoid quotes (especially scare quotes, they are annoying to many readers).} model of edit operations on the phone sequence, which approximates a phonological system. Both the DPHMM and AG systems make use of stochastic processes (the Dirichlet process and the Pitman-Yor process, respectively) which allow for variable numbers of phone, morpheme, and word classes while penalizing overly complex models.\cm{well, the PYP and DP are really basically the same thing. Again, describe on a higher level ``bayesian nonparametrics''} Having flexibility in the number of discoverable units on the phonemic and morphemic level helps the model be language-independent, and is particularly crucial on the word level, since the stacking of suffixes and prefixes means there could be an unbounded number of words in a vocabulary.\cm{You can end this more strongly by saying instead that the model is able to acquire the full stack of linguistic units in a fully unsupervised, language-independent way. You also need to say significantly more about the novel contribution---building on this earlier work we develop a VB inference algorithm, which has thesesuch and such advantages.} \\
\tab We will present some preliminary pilot results from running this model, as well as an overview of the model structure and function. We will also discuss some further extensions of the model.\cm{I need to think a little about what to say now about results....}

\end{document}

