Steps for running on cluster 
----------------------------

1. ssh into your computecanada account, either on cedar or graham (e.g. `ssh <user>@cedar.computecanada.ca`)
	- if you don't have a computecanada account, follow these instructions to get one: https://www.computecanada.ca/research-portal/account-management/apply-for-an-account/
	- Right now the accounts are through Morgan, so contact him for the PI information 
2. clone the repo into your user's folder (should be under `~/project/<user>` once you've ssh'd in)
3. navigate to the home directory and load python 
	- `module load python/3.5.4`
4. create a virtualenv for the project and activate it (or just activate it if you have already created it before)
	- `virtualenv uld`
	- and/or `source ~/uld/bin/activate`
5. navigate to the ULD/code directory and install the requirements 
	- `pip install -r requirements.txt`
6. jobs are submitting/run via scripts. It's fairly trivial to turn the `run_full` script into a script that can run on the cluster. 
	- First, change the name to `run_full.sh` and make sure that the permissions allow it to be executed. 
	- prepend the following chunk: 
	```
	#!/bin/bash
	#SBATCH --account=def-msonde1
	#SBATCH --ntasks=32               # number of MPI processes
	#SBATCH --mem-per-cpu=64G      # memory; default unit is megabytes
	#SBATCH --time=0-36:00          # time (DD-HH:MM)
	```
	- Starting the ipython clusters is a bit trickier. Initially I thought that `ipcluster -n N` would suffice -- however, it turns out that the SLURM scheduler used won't let you start MPI processes like this. In order to start the engines, I followed the instructions found here: https://k-d-w.org/node/96 and here: http://www.makeloft.org/2016/05/work-in-progress-parallel-ipython-from.html. 
	- First, we make a new profile for each job. Creating a new profile avoids conflicts from previous attempts:
	```
	profile=job_${SLURM_JOB_ID}_$(hostname)
	echo "CREATING PROFILE ${profile}";
	ipython profile create ${profile};
	```
	- Next, we tell the ipcontroller to listen to listen on all interfaces. In this case, the interfaces are node IDs and port numbers. The controller is the host that launches and delegates tasks to the engines, so we want it to be able to interace with all nodes and ports. We can do this by adding the following lines to our script: 
	```
	echo "STARTING ENGINES";
	ipcontroller --ip='*' --profile=${profile} --log-to-file &
	sleep 10;
	```

	- We want to start the actual engines on which the computation will take place. We can use the ipengine command for this, which will automatically connect the engines (as many as there are CPUs allocated for the task) to the controller of the same profile.
	```
	srun ipengine --profile=${profile} --location=$(hostname) --log-to-file &
	sleep 45
	echo "ENGINES STARTED"; 
	```
	- Finally, we want to run our code. In this case, we'll run a ULD simulation:
	```
	python -u run_amdtk_nc.py --bottom_plu_count=5 --n_epochs=1 --audio_dir=../audio/sa1 --eval_dir=../audio/sa1 --output_dir=output --batch-size=1 --profile ${profile}
	```
	
	- You can change the number of processes, memory, and expected time. Note that all of these are considered by the scheduler when scheduling your job -- the higher they are (especially time) the later your job gets scheduled. If you want to test something out, it's best to set low time limits and use little memory/few processes until you're sure that your job will run in its entirety. 

7. To submit your job, input `sbatch run_full.sh`. To see the status of your job, you can input `squeue -u <username>`. To cancel a job, input `scancel <jobid>` or `scancel -u <username>` to cancel all your jobs. The stdout output of your job will be in an autogenerated file called `slurm-<jobid>.out`. However, run_full.sh redirects most of the output to debug.txt. 
